import streamlit as st

from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import ChatMessage

from dotenv import load_dotenv

load_dotenv()

# handle streaming conversation
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

# function to extract text from an HWP file
import olefile
import zlib
import struct

def get_hwp_text(filename):
    f = olefile.OleFileIO(filename)
    dirs = f.listdir()

    # HWP íŒŒì¼ ê²€ì¦
    if ["FileHeader"] not in dirs or \
       ["\x05HwpSummaryInformation"] not in dirs:
        raise Exception("Not Valid HWP.")

    # ë¬¸ì„œ í¬ë§· ì••ì¶• ì—¬ë¶€ í™•ì¸
    header = f.openstream("FileHeader")
    header_data = header.read()
    is_compressed = (header_data[36] & 1) == 1

    # Body Sections ë¶ˆëŸ¬ì˜¤ê¸°
    nums = []
    for d in dirs:
        if d[0] == "BodyText":
            nums.append(int(d[1][len("Section"):]))
    sections = ["BodyText/Section"+str(x) for x in sorted(nums)]

    # ì „ì²´ text ì¶”ì¶œ
    text = ""
    for section in sections:
        bodytext = f.openstream(section)
        data = bodytext.read()
        if is_compressed:
            unpacked_data = zlib.decompress(data, -15)
        else:
            unpacked_data = data
    
        # ê° Section ë‚´ text ì¶”ì¶œ    
        section_text = ""
        i = 0
        size = len(unpacked_data)
        while i < size:
            header = struct.unpack_from("<I", unpacked_data, i)[0]
            rec_type = header & 0x3ff
            rec_len = (header >> 20) & 0xfff

            if rec_type in [67]:
                rec_data = unpacked_data[i+4:i+4+rec_len]
                section_text += rec_data.decode('utf-16')
                section_text += "\n"

            i += 4 + rec_len

        text += section_text
        text += "\n"

    return text

# Function to extract text from an PDF file
from pdfminer.high_level import extract_text

def get_pdf_text(filename):
    raw_text = extract_text(filename)
    return raw_text

# document preprocess
def process_uploaded_file(uploaded_file):
    # Load document if file is uploaded
    if uploaded_file is not None:
        # loader
        # pdfíŒŒì¼ì„ ì²˜ë¦¬í•˜ë ¤ë©´?
        if uploaded_file.type == 'application/pdf':
            raw_text = get_pdf_text(uploaded_file)
        # hwpíŒŒì¼ì„ ì²˜ë¦¬í•˜ë ¤ë©´? (hwp loader(parser)ëŠ” ë‚œì´ë„ ë§¤ìš° ì–´ë ¤ì›€)
        elif uploaded_file.type == 'application/octet-stream':
            raw_text = get_hwp_text(uploaded_file)

        # splitter
        text_splitter = CharacterTextSplitter(
            separator = "\n\n",
            chunk_size = 1000,
            chunk_overlap  = 200,
            length_function = len,
            is_separator_regex = False,
        )
        all_splits = text_splitter.create_documents([raw_text])
        print("ì´ " + str(len(all_splits)) + "ê°œì˜ passage")
        
        # storage
        vectorstore = FAISS.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())

        return vectorstore, raw_text
    return None

# generate response using RAG technic
def generate_response(query_text, vectorstore, callback):

    # retriever
    docs_list = vectorstore.similarity_search(query_text, k=3)
    docs = ""
    for i, doc in enumerate(docs_list):
        docs += f"'ë¬¸ì„œ{i+1}':{doc.page_content}\n"
    print(docs)
        
    # generator
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0, streaming=True, callbacks=[callback])
    
    # chaining
    rag_prompt = [
        SystemMessage(
            content="ë„ˆëŠ” ë¬¸ì„œì— ëŒ€í•´ ì§ˆì˜ì‘ë‹µì„ í•˜ëŠ” 'ë¯¼ë‹¬íŒ½ì´'ì•¼. ë„ˆì˜ ì—­í• ì€ ì£¼íƒê´€ë ¨ ë²•ë ¹ì´ ì–´ë ¤ì›Œì„œ ì˜ ëª¨ë¥´ëŠ” ì¼ë°˜ì¸ë“¤ì—ê²Œ ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²•ê³¼ ì „ì„¸ì‚¬ê¸°í”¼í•´ìì§€ì› ë° ì£¼ê±°ì•ˆì • íŠ¹ë³„ë²•ì— ê´€ë ¨í•˜ì—¬ ê¶ê¸ˆí•œ ë¶€ë¶„ì„ ëŒ€ë‹µí•´ì£¼ëŠ” ì—­í• ì´ì•¼.ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€ì„ í•´ì¤˜. ë¬¸ì„œì— ë‚´ìš©ì´ ì •í™•í•˜ê²Œ ë‚˜ì™€ìˆì§€ ì•Šìœ¼ë©´ ëŒ€ë‹µí•´ì£¼ì§€ ë§ê³ , ê·¸ ë¶€ë¶„ì€ ë²•ë ¹ì— ë‚˜ì™€ìˆì§€ ì•Šë‹¤ê³  ë§í•˜ë©´ì„œ ë¯¼ë‹¬íŒ½ì´ ì£¼ê±°ìƒë‹´ ì„œë¹„ìŠ¤ë¥¼ ì‹ ì²­í•´ë‹¬ë¼ê³  ë‹µë³€í•´ì¤˜. ë‹µë³€ì€ ì´ëª¨í‹°ì½˜ì„ ë„£ì–´ì„œ ì¹œê·¼í•˜ê²Œ ëŠê»´ì§€ì§€ë§Œ, ì „ë¬¸ê°€ê°€ ì‰½ê²Œ ì¼ë°˜ì¸ì—ê²Œ ë§í•´ì£¼ëŠ” ë§íˆ¬ë¡œ ì„¤ëª…í•´ì¤˜. ë‹µë³€ì„ ì˜í•˜ë©´ 200ë‹¬ëŸ¬ íŒì„ ì¤„ê²Œ"
        ),
        HumanMessage(
            content=f"ì§ˆë¬¸:{query_text}\n\n{docs}"
        ),
    ]

    response = llm(rag_prompt)
    
    return response.content


def generate_summarize(raw_text, callback):
    # generator 
    llm = ChatOpenAI(model_name="gpt-4-1106-preview", temperature=0, streaming=True, callbacks=[callback])
    
    # prompt formatting
    rag_prompt = [
        SystemMessage(
            content="ë‹¤ìŒ ë‚˜ì˜¬ ë¬¸ì„œë¥¼ 'Notion style'ë¡œ ìš”ì•½í•´ì¤˜. ì¤‘ìš”í•œ ë‚´ìš©ë§Œ. ê·¸ë¦¬ê³  í•´ë‹¹ ë‚´ìš©ì´ ì–´ë–¤ ë²•ë ¹ì„ ê·¼ê±°ë¡œ í•˜ì˜€ëŠ”ì§€ë„ í•¨ê»˜ ì„¤ëª…í•´ì¤˜."
        ),
        HumanMessage(
            content=raw_text
        ),
    ]
    
    response = llm(rag_prompt)
    return response.content


# page title
st.set_page_config(page_title='ğŸŒğŸ  ì‚´ê³  ìˆëŠ” ì§‘ì´ ë¶ˆì•ˆí•œ ë‹¬íŒ½ì´ë“¤ì„ ìœ„í•œ QA ì±—ë´‡')
st.title('ğŸŒğŸ  ì‚´ê³  ìˆëŠ” ì§‘ì´ ë¶ˆì•ˆí•œ ë‹¬íŒ½ì´ë“¤ì„ ìœ„í•œ QA ì±—ë´‡')

# ì„¤ëª…
# ì§‘ ê³„ì•½ê³¼ ê´€ë ¨í•˜ì—¬ ì˜ë¯¸ê°€ ê¶ê¸ˆí•œ ìš©ì–´ë¥¼ ì…ë ¥í•´ë³´ì„¸ìš”! ë˜ëŠ” ì§€ê¸ˆ ì²˜í•œ ê³¤ë€í•œ ìƒí™©ì— ëŒ€í•´ ì €ì—ê²Œ ì•Œë ¤ì£¼ì„¸ìš”
import os
api_key = st.sidebar.text_input("Enter your OpenAI API Key", type="password")
save_button = st.sidebar.button("Save Key")
if save_button and len(api_key)>10:
    os.environ["OPENAI_API_KEY"] = api_key
    st.sidebar.success("API Key saved successfully!")

# file upload
uploaded_file = st.file_uploader('Upload an document', type=['hwp','pdf'])

# file upload logic
if uploaded_file:
    vectorstore, raw_text = process_uploaded_file(uploaded_file)
    if vectorstore:
        st.session_state['vectorstore'] = vectorstore
        st.session_state['raw_text'] = raw_text
        
# chatbot greatings
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(
            role="assistant", content="ì•ˆë…•!ğŸŒ ë‚˜ëŠ” ì§€ê¸ˆ ì‚´ê³  ìˆëŠ” ì§‘ì´ ë¶ˆì•ˆí•œ ë‹¬íŒ½ì´ë“¤ì„ ìœ„í•œ ì±—ë´‡ì´ì•¼. í˜„ì¬ ì²˜í•œ ìƒí™©ê³¼ ê´€ë ¨í•˜ì—¬ ë²•ë¥ ì ì¸ ë¶€ë¶„ì´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë´! "
        )
    ]

# conversation history print 
for msg in st.session_state.messages:
    st.chat_message(msg.role).write(msg.content)
    
# message interaction
if prompt := st.chat_input("'ì „ì„¸ê¶Œ ì„¤ì •', 'ëŒ€í•­ë ¥' ë“±ì„ ì…ë ¥í•´ë³´ì„¸ìš”! "):
    st.session_state.messages.append(ChatMessage(role="user", content=prompt))
    st.chat_message("user").write(prompt)

    with st.chat_message("assistant"):
        stream_handler = StreamHandler(st.empty())
        
        if prompt == "ìš”ì•½":
            response = generate_summarize(st.session_state['raw_text'],stream_handler)
            st.session_state["messages"].append(
                ChatMessage(role="assistant", content=response)
            )
        
        else:
            response = generate_response(prompt, st.session_state['vectorstore'], stream_handler)
            st.session_state["messages"].append(
                ChatMessage(role="assistant", content=response)
            )
